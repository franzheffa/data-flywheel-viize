{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Discover Cost-Efficient AI Customer Service Agents with NVIDIA Data Flywheel Blueprint\n",
    "[![ Click here to deploy.](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-2wggjBvDlVp4pLQD8ytZySh5m8W)\n",
    "\n",
    "A data flywheel is a feedback loop where data collected from interactions or processes is used to continuously refine AI models, which in turn generates better outcomes and more valuable data. In this notebook, you will learn how to use the Data Flywheel Foundational Blueprint and the [Agent Intelligence (AIQ) toolkit](https://docs.nvidia.com/aiqtoolkit/latest/index.html) to continuously discover and promote more cost-efficient agents for an [AI virtual customer service assistant](https://build.nvidia.com/nvidia/ai-virtual-assistant-for-customer-service). The [Agent Intelligence (AIQ) toolkit](https://docs.nvidia.com/aiqtoolkit/latest/index.html) is a flexible, lightweight, and unifying library that allows you to easily connect existing enterprise agents to data sources and tools across any framework. For more information, please reference the [documentation](https://docs.nvidia.com/aiqtoolkit/latest/index.html) and [GitHub project](https://github.com/NVIDIA/AIQToolkit/tree/develop).\n",
    "\n",
    "### Data Flywheel Blueprint\n",
    "\n",
    "![Data Flywheel Blueprint](../docs/images/data-flywheel-blueprint.png)\n",
    "\n",
    "\n",
    "### AI Virtual Assistant for Customer Service\n",
    "\n",
    "The primary customer service agent in the AI Virtual Assistant uses tool calling to route user queries to specialized assistants, including: \n",
    "\n",
    "- Product Q&A\n",
    "- Order status verification\n",
    "- Returns processing\n",
    "- Small talk and casual engagement\n",
    "\n",
    "The [Agent Intelligence toolkit](https://docs.nvidia.com/aiqtoolkit/latest/index.html) will automatically collect and transmit runtime logs and tool-calling data from these interactions to Data Flywheel, enabling you to leverage them as evaluation benchmarks and training data.\n",
    "In this tutorial, you'll use this information to drive the flywheel process, fine-tuning smaller LLMs (such as `meta/llama-3.2-1B-instruct`, `meta/llama-3.2-3B-instruct`, `meta/llama-3.1-8B-instruct`) to match accuracy of the currently deployed model (`meta/llama-3.3-70B-instruct`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing with the Blueprint\n",
    "\n",
    "The following diagram illustrates how admin tools and applications interact with the Flywheel Blueprint, which orchestrates logging, processing, and model management to enable continuous optimization.\n",
    "\n",
    "![Arch](arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents \n",
    "\n",
    "0. [Data Flywheel Setup](#0)\n",
    "1. [Interact with baseline AI Virtual Assistant](#1)\n",
    "2. [Runtime Data Flywheel Ingestion](#2)\n",
    "3. [Create a Flywheel Job](#3)\n",
    "4. [Monitor Job Status](#4)\n",
    "5. [Redeploy AI Virtual Assistant](#5)\n",
    "6. [Optional: Show Continuous Improvement](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## Data Flywheel Setup\n",
    "\n",
    "The Data Flywheel service is built on top of the [NeMo Microservices](https://docs.nvidia.com/nemo/microservices/latest/about/index.html). Before setting up the DataFlywheel service, ensure that NeMo Microservices is already deployed in your environment — it serves as a prerequisite for this workflow.\n",
    "\n",
    "The DataFlywheel service itself is packaged as a set of Docker containers and can be brought up using Docker Compose.\n",
    "\n",
    "In general, you can set up the Data Flywheel service by following the instructions provided in the [Quick Start Guide](https://github.com/NVIDIA-AI-Blueprints/data-flywheel/blob/main/docs/02-quickstart.md). \n",
    "\n",
    "\n",
    "If you want to quickly spin up the DataFlywheel service with minimal configuration, we recommend starting with the [Data Flywheel Blueprint Brev Launchable](https://brev.nvidia.com/launchable/deploy/now?launchableID=env-2wggjBvDlVp4pLQD8ytZySh5m8W) (see instructions below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA Brev Launchable Setup Instructions\n",
    "\n",
    "> **Important:** The instructions below apply **only** to users running this notebook via the Brev Launchable.\n",
    "\n",
    "NVIDIA Brev is a developer-friendly platform that makes it easy to run, train, and deploy ML models on cloud GPUs without the hassle of setup—it comes preloaded with Python, CUDA, and Docker so you can get started fast. \n",
    "\n",
    "Brev Launchables are shareable, pre-preconfigured GPU environments that bundle your code, containers, and compute into one easy-to-launch link.\n",
    "\n",
    "Please follow the steps below if you are using this notebook as part of the Brev Launchable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Set API Keys - [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#generating-api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NGC_API_KEY'] = '<your_ngc_api_key>'\n",
    "os.environ['NVIDIA_API_KEY'] = '<your_nim_api_key>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Clone the data flywheel repo and fetch data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/NVIDIA-AI-Blueprints/data-flywheel.git\n",
    "cd data-flywheel\n",
    "sudo apt-get update && sudo apt-get install -y git-lfs\n",
    "git lfs install\n",
    "git-lfs pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Set up paths and installs python dependencies for notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir / \"data-flywheel\"\n",
    "data_dir = project_root / \"data\"\n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory changed to: {Path.cwd()}\")\n",
    "\n",
    "user_site = Path.home() / \".local\" / \"lib\" / f\"python{sys.version_info.major}.{sys.version_info.minor}\" / \"site-packages\"\n",
    "if str(user_site) not in sys.path:\n",
    "    sys.path.append(str(user_site))\n",
    "    print(f\"Added user site-packages to sys.path: {user_site}\")\n",
    "\n",
    "%pip install --user elasticsearch==8.17.2 pydantic-settings>=2.9.1 pandas>=2.2.3 matplotlib==3.10.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries and configure pandas display options for better readability in notebook outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Width of the display in characters\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content of each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Update `config/config.yaml` to use remote LLM as judge. By default, data flywheel blueprint deploys `LLama-3.3-70B-instruct` locally for LLM as a judge, which requires 4 GPUs. But for the launchable, we will choose the remote LLM judge and use the `LLama-3.3-70B-instruct` NIM hosted on [build.nvidia.com](https://build.nvidia.com/meta/llama-3_3-70b-instruct).\n",
    "\n",
    "By default, only `Llama-3.2-1b-instruct` will be used in the flywheel but you can uncomment other models in the yaml file to include in the flywheel run. You can also change other config settings such as data split and training hyperparameters as desired\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textwrap import dedent\n",
    "\n",
    "config_path = project_root / \"config\" / \"config.yaml\"\n",
    "\n",
    "new_llm_block = dedent(\"\"\"\\\n",
    "llm_judge_config:\n",
    "  type: \"remote\"\n",
    "  url: \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "  model_id: \"meta/llama-3.3-70b-instruct\"\n",
    "  api_key_env: \"NGC_API_KEY\"\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "new_nims_block = dedent(\"\"\"\\\n",
    "nims:\n",
    "  - model_name: \"meta/llama-3.2-1b-instruct\"\n",
    "    context_length: 8192\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "\n",
    "  - model_name: \"meta/llama-3.2-3b-instruct\"\n",
    "    context_length: 32768\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "\n",
    "  - model_name: \"meta/llama-3.1-8b-instruct\"\n",
    "    context_length: 32768\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "text = config_path.read_text()\n",
    "\n",
    "def replace_block(yaml_text: str, key: str, new_block: str) -> str:\n",
    "    pattern = rf\"(?ms)^({re.escape(key)}:[\\s\\S]*?)(?=^\\S|\\Z)\"\n",
    "    return re.sub(pattern, new_block, yaml_text)\n",
    "\n",
    "text = replace_block(text, \"llm_judge_config\", new_llm_block)\n",
    "text = replace_block(text, \"nims\",              new_nims_block)\n",
    "\n",
    "config_path.write_text(text)\n",
    "print(\"config.yaml updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Start data flywheel service, which involves first deploying the Nemo Microservices and then bring up the data flywheel service via docker compose. This step take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "log() {\n",
    "  echo -e \"\\033[1;32m[INFO]\\033[0m $1\"\n",
    "}\n",
    "\n",
    "echo \"$NGC_API_KEY\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "chmod +x scripts/deploy-nmp.sh scripts/run.sh\n",
    "\n",
    "log \"Starting Nemo Microservices deployment...\"\n",
    "./scripts/deploy-nmp.sh >> flywheel_deploy.log 2>&1\n",
    "log \"NMP deployed successfully!\"\n",
    "\n",
    "log \"Starting data flywheel service...\"\n",
    "./scripts/run.sh >> flywheel_deploy.log 2>&1\n",
    "log \"Data flywheel service started successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6**: Install the AI Virtual Assistant Blueprint. This will take a few minutes to pull down and build containers the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone repository\n",
    "git clone --branch aiq-dfw-integration https://github.com/NVIDIA-AI-Blueprints/ai-virtual-assistant.git\n",
    "cd ai-virtual-assistant\n",
    "# Install blueprint\n",
    "echo \"Building api-gateway-server...\"\n",
    "docker compose -f deploy/compose/docker-compose.aiq.yaml build api-gateway-server --quiet\n",
    "echo \"Building agent-chain-server...\"\n",
    "docker compose -f deploy/compose/docker-compose.aiq.yaml build agent-chain-server --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download datasets\n",
    "cd ai-virtual-assistant\n",
    "echo \"Downloading datasets...\"\n",
    "echo \"Ingesting datasets...\"\n",
    "docker compose -f src/ingest_service/docker-compose.yaml run ingest-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "## Step 1: Interact with baseline AI Virtual Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an AIQ toolkit based agent, the AI Virtual Assistant can be deployed by YAML configuration and is seamlessly integrated with Data Flywheel. The most salient sections of this YAML file are illustrated below.\n",
    "\n",
    "```yaml\n",
    "general:\n",
    "  use_uvloop: true\n",
    "  logging:\n",
    "    console:\n",
    "      _type: console\n",
    "      level: WARN\n",
    "\n",
    "  telemetry:\n",
    "    tracing: \n",
    "      dfw_elasticsearch:  # <- Simple configuration to ship runtime logs to Data Flywheel\n",
    "        _type: nemo_dfw_elasticsearch\n",
    "        endpoint: ${DATA_FLYWHEEL_ENDPOINT}\n",
    "        client_id: ${CLIENT_ID}\n",
    "        index: flywheel\n",
    "      phoenix:  # <- Traces will also ship to a running Phoenix server\n",
    "        _type: phoenix\n",
    "        endpoint: ${PHOENIX_ENDPOINT}\n",
    "        project: ai-virtual-assistant        \n",
    "\n",
    "... # Omitting some sections for brevity\n",
    "\n",
    "llms:\n",
    "  tool_call_llm:\n",
    "    _type: nim\n",
    "    model_name: ${APP_TOOLCALL_LLM_MODELNAME}  # <- We start with a meta/llama-3.3-70b-instruct model\n",
    "    base_url: ${APP_TOOLCALL_LLM_SERVERURL}\n",
    "    temperature: 0.2\n",
    "    top_p: 0.7\n",
    "    max_tokens: 1024    \n",
    "    api_key: ${NVIDIA_API_KEY}\n",
    "  chat_llm:\n",
    "    _type: nim\n",
    "    model_name: ${APP_CHAT_LLM_MODELNAME}\n",
    "    base_url: ${APP_CHAT_LLM_SERVERURL}\n",
    "    temperature: 0.2\n",
    "    top_p: 0.7\n",
    "    max_tokens: 1024    \n",
    "    api_key: ${NVIDIA_API_KEY}    \n",
    "\n",
    "workflow:\n",
    "  _type: aiva_agent\n",
    "  tool_call_llm_name: tool_call_llm\n",
    "  chat_llm_name: chat_llm\n",
    "\n",
    "```\n",
    "\n",
    "Deployment environment variables are passed through to the AIQ toolkit configuration object to drive runtime configuration settings. There are two important sections of this file that enable the Data Flywheel to improve application performance:\n",
    "\n",
    "1. **Data Flywheel Integration** - AIQ toolkit provides configurable Data Flywheel integrations, allowing developers to stream runtime logs effortlessly and without additional development costs.\n",
    "2. **LLM selection** - Configure the optimal LLM for your Agentic application based on insights from Data Flywheel, allowing for tailored performance and efficiency gains. Defaulting to meta/llama-3.3-70b-instruct, but easily adaptable to lighter-weight models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deploy the Baseline AI Virtual Assistant**\n",
    "\n",
    "Now, let's deploy AI Virtual Assistant agent using a `meta/llama-3.3-70b-instruct` model for both tool calling and general chat completions tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ai-virtual-assistant\n",
    "# Set our deployment parameters\n",
    "export APP_TOOLCALL_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "export APP_CHAT_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "export APP_LLM_MODELENGINE=nvidia-ai-endpoints\n",
    "export DFW_CLIENT_ID=aiq-ai-virtual-assistant\n",
    "\n",
    "# Bring down the current AI Virtual Assistant agent service\n",
    "echo \"Bringing down the agent-chain-server\"\n",
    "docker compose -f deploy/compose/docker-compose.aiq.yaml down agent-chain-server\n",
    "# Redeploy the AI Virtual Assistant\n",
    "echo \"Redeploying the agent-chain-server\"\n",
    "docker compose -f deploy/compose/docker-compose.aiq.yaml up --quiet-pull -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing with the Blueprint\n",
    "![AIVA-UI](img/aiva-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now interact with AI Virtual Assistant through its user interface hosted at [http://localhost:3001](http://localhost:3001). A Phoenix server will also be available at [http://localhost:6006](http://localhost) for an intuitive view of all runtime traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"2\"></a>\n",
    "## Step 2: Runtime Data Flywheel Ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have been interacting with the AI Virtual Assistant application, runtime traces have been transmitted to Data Flywheel. These data points are considered **ground truth**.\n",
    "\n",
    "Ground truth data points are used to **evaluate** and **customize** more efficient models that can perform similarly to the current model. This customization process is analogous to a student-teacher distillation setup, where synthetic data generated from the teacher model is used to fine-tune a student model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data point has the following schema:\n",
    "\n",
    "| Field        | Type               | Description                                                         |\n",
    "|--------------|--------------------|---------------------------------------------------------------------|\n",
    "| `timestamp`  | `int` (epoch secs) | Time the request was issued                                         |\n",
    "| `workload_id`| `str`              | Stable identifier for the logical task / route / agent node         |\n",
    "| `client_id`  | `str`              | Identifier of the application or deployment that generated traffic  |\n",
    "| `request`    | `dict`             | Exact [`openai.ChatCompletion.create`](https://platform.openai.com/docs/api-reference/chat/create) payload received by the model |\n",
    "| `response`   | `dict`             | Exact `ChatCompletion` response returned by the model               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `request` uses the OpenAI `ChatCompletions` request format and contains the following attributes:\n",
    "\n",
    "- `model` includes the Model ID used to generate the response.\n",
    "- `messages` includes a `system` message as well as a `user` query.\n",
    "- `tools` includes a list of functions and parameters available to the LLM to choose from, as well as their parameters and descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the most recent AI Virtual Assistant runtime trace that has been ingested into Data Flywheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://localhost:9200/flywheel/_search\" \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"size\": 1, \"sort\": [{\"timestamp\": {\"order\": \"desc\"}}]}' | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great! In production, we would likely be collecting data for an extended period. To speed up the process, let's simulate collectioning these traces with a bulk ingestion so we can kick off the Data Flywheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.load_test_data import load_data_to_elasticsearch\n",
    "\n",
    "load_data_to_elasticsearch(file_path=\"aiva_primary_assistant_dataset.jsonl\", workload_id=\"aiva_agent\", client_id=\"aiq-ai-virtual-assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"3\"></a>\n",
    "## Step 3: Create a Flywheel Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a Flywheel job by sending a POST request to the `/jobs` API. This triggers the workflow asynchronously.\n",
    "\n",
    "In production environments, you can automate this process to run at scheduled intervals, in response to specific events, or on demand.\n",
    "\n",
    "For this tutorial, we will target the primary customer service agent by setting the `workload_id` to \"aiva_agent\" and we will set `client_id` to \"aiq-ai-virtual-assistant\" which has 5079 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flywheel Orchestrator URL\n",
    "API_BASE_URL = \"http://0.0.0.0:8000\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": \"aiva_agent\", \"client_id\": \"aiq-ai-virtual-assistant\"}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"4\"></a>\n",
    "## Step 4: Monitor Job Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a GET request to `/jobs/{job_id}` to retrieve the current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_job_status(job_id):\n",
    "    \"\"\"Get the current status of a job.\"\"\"\n",
    "    response = requests.get(f\"{API_BASE_URL}/api/jobs/{job_id}\")\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job_status(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the job status output, you will see the following metrics for evaluating the accuracy of tool calling:\n",
    "\n",
    "- `function_name_accuracy`: This metric evaluates whether the LLM correctly predicts the function name.\n",
    "    - **Definition**: It checks for an **exact match** between the predicted function name and the ground truth function name.\n",
    "    - **Scoring**:\n",
    "        - `1` if the predicted function name exactly matches the ground truth.\n",
    "        - `0` otherwise.\n",
    "\n",
    "- `function_name_and_args_accuracy (exact-match)`: This stricter metric checks whether **both** the function name and all associated function arguments are correctly predicted.\n",
    "    - **Definition**: The prediction is considered correct **only** if:\n",
    "        - The function name is an **exact match**, and\n",
    "        - Every function argument is also an **exact match** to the ground truth.\n",
    "    - **Scoring**:\n",
    "        - `1` if both the function name and all arguments exactly match.\n",
    "        - `0` otherwise.\n",
    "\n",
    "    This measures the LLM's ability to generate an entirely accurate function call, including both the correct operation and the exact input values.\n",
    "\n",
    "- `function_name_and_args_accuracy (LLM-judge)`: This metric uses a LLM to act as a \"judge\" and assess the correctness of the function call based on semantic meaning, particularly useful when arguments are complex or naturally rephrased.\n",
    "    - **Definition**:\n",
    "        - The function name must be an **exact match**.\n",
    "        - For function arguments:\n",
    "            - If an argument is simple and expected to match exactly (e.g., a user ID or fixed keyword), it must be an **exact match**.\n",
    "            - If an argument is more complex (e.g., a user query or free-text input), **semantic similarity** is evaluated using the LLM-as-judge.\n",
    "    - **Scoring**:\n",
    "        - `1` if all criteria (function name match, and each argument passing either the exact match or semantic check) are satisfied.\n",
    "        - `0` otherwise.\n",
    "\n",
    "    This metric captures functional correctness even when the LLM rewrites or paraphrases input arguments, as long as the **intent and outcome remain accurate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process and enable continuous monitoring, we defined a utility function `monitor_job` in `job_monitor_helper.py`:\n",
    "\n",
    "- Periodically retrieve the job status\n",
    "- Format the output into a table\n",
    "\n",
    "This makes it easier to compare and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.job_monitor_helper import monitor_job\n",
    "\n",
    "# Start monitoring the job with polling interval of 5s\n",
    "monitor_job(api_base_url=API_BASE_URL, job_id=job_id, poll_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve now successfully completed a Flywheel run and can review the evaluation results to decide whether to promote the model. However, with only 300 data points, the customized `Llama-3.2-1B-instruct` is likely still limited in performance.\n",
    "\n",
    "That said, the Data Flywheel operates as a self-reinforcing cycle—models continue to improve as more user interaction logs are collected. Below, we demonstrate how the model performance improves incrementally with additional data. \n",
    "\n",
    "Note that the Eval metrics shown in the figures are the `function_name_accuracy`.\n",
    "\n",
    "\n",
    "![300dp](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/main/notebooks/img/300dp.png)\n",
    "\n",
    "\n",
    "**Flywheel run results at 300 data points**\n",
    "\n",
    "![500dp](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/main/notebooks/img/500dp.png)\n",
    "\n",
    "**Flywheel run results at 500 data points**\n",
    "\n",
    "![1000dp](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/main/notebooks/img/1000dp.png)\n",
    "\n",
    "**Flywheel run results at 1,000 data points**\n",
    "\n",
    "With the improvement results demonstrated, you can now move on to Step 4 to run the Flywheel with additional data yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"5\"></a>\n",
    "## Step 5: Redeploy AI Virtual Assistant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deploy the Customized NIM with the AI Virtual Assistant**\n",
    "The flywheel job has trained LoRA adapters for our LLMs that can be redeployed as NIMs in our local cluster. Follow the steps to deploy this model with the AI Virtual Assistant application.\n",
    "\n",
    "**Step 1**: Identify the model based on evaluation criteria.\n",
    "\n",
    "**Step 2**: Identify the selected model's customization `nmp_uri`. It will have the following format:\n",
    "\n",
    "`http://nemo.test/v1/customization/jobs/<CUSTOMIZATION_ID>`\n",
    "\n",
    "Grab the `CUSTOMIZATION_ID` from this ouput.\n",
    "\n",
    "**Step 3**: Deploy the NIM and corresponding LoRA adapter(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl --location 'http://nemo.test/v1/deployment/model-deployments'\n",
    "     --header 'Content-Type: application/json'\n",
    "     --data '{\n",
    "     \"name\": \"<YOUR_MODEL_NAME>\",\n",
    "     \"namespace\": \"meta\",\n",
    "     \"config\": {\n",
    "        \"model\": \"<YOUR_MODEL_NAME>\",\n",
    "        \"nim_deployment\": {\n",
    "           \"image_name\": \"nvcr.io/nim/<YOUR_MODEL_NAME>\",\n",
    "           \"image_tag\": \"1.8.3\",\n",
    "           \"pvc_size\": \"25Gi\",\n",
    "           \"gpu\": 1,\n",
    "           \"additional_envs\": {}\n",
    "           }\n",
    "        }\n",
    "     }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally delete the model deployment\n",
    "# curl -X DELETE 'http://nemo.test/v1/deployment/model-deployments/<MODEL_NAMESPACE>\\<MODEL_NAME>'\n",
    "# EXAMPLE: curl -X DELETE 'http://nemo.test/v1/deployment/model-deployments/meta\\meta\\/llama-3.2-1b-instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Update the configuration and redeploy the AI Virtual Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export APP_LLM_MODELNAME=dfwbp/customized-<YOUR_MODEL_NAME>@<CUSTOMIZATION_ID>\n",
    "export APP_LLM_MODELENGINE=nvidia-ai-endpoints\n",
    "export APP_LLM_SERVERURL=http://nim.test/v1/chat/completions\n",
    "export DFW_CLIENT_ID=aiq-aiva-customized\n",
    "\n",
    "# Bring down the current AI Virtual Assistant agent service\n",
    "echo \"Bringing down the agent-chain-server\"\n",
    "docker compose -f deploy/compose/docker-compose.aiq.yaml down agent-chain-server\n",
    "# Redeploy the AI Virtual Assistant\n",
    "echo \"Redeploying the agent-chain-server\"\n",
    "docker compose -f deploy/compose/docker-compose.aiq.yaml up --quiet-pull -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Experience the updated application hosted at [http://localhost:3001](http://localhost:3001) \n",
    "\n",
    "This time we are using a much smaller model, while preserving the quality of outputs! In summary, the our AIQ tookit based application has streamed groud truth traces directly to flywheel. Flywheel has enabled us to discover more optimimal models for our application. Redeploying this application provides a more cost optimized deployment and improved end user experience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are using a much smaller model, while preserving the quality of outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **NVIDIA Agent Intelligence (AIQ) toolkit**, combined with a **data flywheel**, enables organizations to develop cost-effective, personalized, and adaptive AI applications that drive business success. Key takeaways from our experience include:\n",
    "\n",
    "1. The Agent Intelligence (AIQ) toolkit provides a flexible and lightweight foundation for deploying production-ready agentic AI,\n",
    "2. A data flywheel is essential for continuous improvement and adaptability, allowing organizations to refine offerings and meet tangible goals, and\n",
    "3. By leveraging the AIQ toolkit and data flywheel, we were able to deploy a more cost-optimized application with improved end-user experience, while preserving output quality, demonstrating the potential for significant business benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Show Continuous Improvement (Optional)\n",
    "\n",
    "To extend the flywheel run with additional data, consider running the AIVA application for an extended period of time and/or bulk ingest some additional simulated runtime data to evaluate the impact of increased data volume on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\":  <YOUR_WORKLOAD_ID>, \"client_id\": <YOUR_CLIENT_ID>}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_job(api_base_url=API_BASE_URL, job_id=job_id, poll_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some improvements of the customized model compared to the last run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have now collected even more data points, let's kick off another flywheel run by setting `client_id` to \"aiva-3\" which includes **1,000** records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": <YOUR_WORKLOAD_ID>, \"client_id\": <YOUR_CLIENT_ID>}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_job(api_base_url=API_BASE_URL, job_id=job_id, poll_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run with 1,000 data points, we should observe the customized model’s score approaching 1.0. This indicates that the `LLama-3.2-1B-instruct` model achieves accuracy comparable to the much larger `LLama-3.3-70B-instruct` base model deployed in AI Virtual Assistant, while significantly reducing latency and compute usage thanks to its smaller size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
